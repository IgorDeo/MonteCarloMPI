# MPI: Distribu√≠do em Rede vs Execu√ß√£o Local

## Vis√£o Geral

O **MPI (Message Passing Interface)** foi projetado especificamente para **computa√ß√£o distribu√≠da** em m√∫ltiplas m√°quinas conectadas em rede. N√£o √© limitado a threads ou execu√ß√£o local.

## üîÑ **MPI vs Threads**

### MPI (Message Passing)
- ‚úÖ **Processos independentes** (n√£o threads)
- ‚úÖ **Mem√≥ria distribu√≠da** (cada processo tem sua pr√≥pria mem√≥ria)
- ‚úÖ **Comunica√ß√£o via mensagens**
- ‚úÖ **Escal√°vel para clusters e supercomputadores**
- ‚úÖ **Funciona em m√∫ltiplas m√°quinas**

### Threads (OpenMP/pthreads)
- üîÑ **Threads compartilham mem√≥ria**
- üîÑ **Limitado a uma √∫nica m√°quina**
- üîÑ **Comunica√ß√£o via mem√≥ria compartilhada**
- üîÑ **Mais simples, mas menos escal√°vel**

## üåê **Configura√ß√£o MPI Distribu√≠do**

### 1. **Execu√ß√£o Local (Single Node)**
```bash
# Executando em uma m√°quina (8 cores)
mpirun -np 8 ./programa

# O que acontece:
# - 8 processos MPI na mesma m√°quina
# - Cada processo em um n√∫cleo diferente
# - Comunica√ß√£o via mem√≥ria compartilhada (mais r√°pida)
```

### 2. **Execu√ß√£o Distribu√≠da (Multi-Node)**

#### Arquivo de Hosts (`hostfile`)
```bash
# Criar arquivo com lista de m√°quinas
echo "192.168.1.10 slots=4" > hostfile
echo "192.168.1.11 slots=4" >> hostfile
echo "192.168.1.12 slots=8" >> hostfile

# Executar em m√∫ltiplas m√°quinas
mpirun -np 16 --hostfile hostfile ./programa
```

#### Configura√ß√£o SSH (sem senha)
```bash
# Gerar chave SSH (se n√£o existir)
ssh-keygen -t rsa -b 2048

# Copiar chave para m√°quinas remotas
ssh-copy-id user@192.168.1.10
ssh-copy-id user@192.168.1.11
ssh-copy-id user@192.168.1.12

# Testar conex√£o
ssh user@192.168.1.10 "echo 'Conex√£o OK'"
```

#### Exemplo de Execu√ß√£o Distribu√≠da
```bash
# 16 processos distribu√≠dos em 3 m√°quinas
mpirun -np 16 \
       --host 192.168.1.10,192.168.1.11,192.168.1.12 \
       ./monte_carlo_pi 10000000

# Ou usando hostfile
mpirun -np 16 --hostfile hostfile ./monte_carlo_pi 10000000
```

## üèóÔ∏è **Arquiteturas Suportadas**

### 1. **Single Machine (SMP)**
- M√∫ltiplos processos em uma m√°quina
- Comunica√ß√£o via mem√≥ria compartilhada
- Ideal para desenvolvimento e testes

### 2. **Cluster (Distributed Memory)**
- M√∫ltiplas m√°quinas conectadas por rede
- Comunica√ß√£o via rede (Ethernet, InfiniBand)
- Escal√°vel para milhares de n√≥s

### 3. **Hybrid (MPI + OpenMP)**
- MPI entre n√≥s, OpenMP dentro de cada n√≥
- Combina vantagens de ambos os modelos
- Comum em supercomputadores modernos

## üîß **Configura√ß√£o Pr√°tica**

### Verificar Capacidades do Sistema
```bash
# N√∫mero de cores por m√°quina
sysctl -n hw.ncpu  # macOS
nproc              # Linux

# Informa√ß√µes sobre aloca√ß√£o MPI
mpirun --display-allocation --np 4 hostname

# Mapear processos para cores
mpirun --display-map --np 8 hostname
```

### Teste de Conectividade
```bash
# Testar MPI em m√∫ltiplas m√°quinas (exemplo)
mpirun --host localhost,remote-host --np 4 hostname

# Verificar lat√™ncia de rede
mpirun --host host1,host2 --np 2 ./benchmark_latencia
```

## üìä **Performance: Local vs Distribu√≠do**

### Comunica√ß√£o Local (Intra-node)
- **Lat√™ncia**: ~1-10 microsegundos
- **Bandwidth**: ~10-100 GB/s
- **Mecanismo**: Mem√≥ria compartilhada, pipes

### Comunica√ß√£o Distribu√≠da (Inter-node)
- **Lat√™ncia**: ~1-100 milissegundos
- **Bandwidth**: ~1-100 GB/s (depende da rede)
- **Mecanismo**: TCP/IP, InfiniBand, Omni-Path

## üåü **Vantagens do MPI Distribu√≠do**

1. **Escalabilidade Massiva**
   - Milhares de n√≥s
   - Milh√µes de cores
   - Petabytes de mem√≥ria

2. **Toler√¢ncia a Falhas**
   - Isolamento de processos
   - Falha de um n√≥ n√£o afeta outros

3. **Flexibilidade**
   - Heterogeneidade de hardware
   - Diferentes arquiteturas

4. **Padr√£o Industrial**
   - Usado em TOP500 supercomputers
   - Bibliotecas otimizadas (Intel MPI, MVAPICH)

## üöÄ **Exemplo: Cluster Caseiro**

### Configura√ß√£o com Raspberry Pi
```bash
# 4 Raspberry Pi conectados por Wi-Fi
# hostfile
pi1 slots=4
pi2 slots=4  
pi3 slots=4
pi4 slots=4

# Executar Monte Carlo distribu√≠do
mpirun -np 16 --hostfile hostfile ./monte_carlo_pi 100000000
```

### Configura√ß√£o com Docker
```bash
# Criar cluster MPI com Docker
docker-compose up mpi-cluster

# Executar programa distribu√≠do
docker exec mpi-master mpirun --hostfile /etc/hostfile \
                              -np 8 ./monte_carlo_pi 10000000
```

## üîç **Debugging Distribu√≠do**

### Vari√°veis de Ambiente √öteis
```bash
export OMPI_DEBUG=1                    # Debug geral
export OMPI_DEBUG_VERBOSE=1            # Verbose
export OMPI_MCA_btl_base_verbose=1     # Comunica√ß√£o
export OMPI_MCA_plm_base_verbose=1     # Process management
```

### Comandos de Diagn√≥stico
```bash
# Verificar conectividade
mpirun --host host1,host2 --np 2 hostname

# Testar largura de banda
mpirun --host host1,host2 --np 2 ./mpi_bandwidth_test

# Verificar lat√™ncia
mpirun --host host1,host2 --np 2 ./mpi_latency_test
```

## üìù **Resumo**

| Aspecto | Local (SMP) | Distribu√≠do (Cluster) |
|---------|-------------|----------------------|
| **M√°quinas** | 1 | M√∫ltiplas |
| **Mem√≥ria** | Compartilhada | Distribu√≠da |
| **Comunica√ß√£o** | R√°pida | Rede (mais lenta) |
| **Escalabilidade** | Limitada | Massiva |
| **Complexidade** | Baixa | Alta |
| **Casos de Uso** | Desenvolvimento | Produ√ß√£o cient√≠fica |

**Conclus√£o**: MPI pode e deve ser usado tanto localmente quanto distribu√≠do. Sua verdadeira for√ßa est√° na capacidade de escalar para clusters e supercomputadores, permitindo resolver problemas computacionalmente intensivos que n√£o caberiam em uma √∫nica m√°quina. 