# Projeto MPI: CÃ¡lculo de Pi usando Monte Carlo

## DescriÃ§Ã£o do Problema

Este projeto implementa o cÃ¡lculo do valor de Pi (Ï€) utilizando o mÃ©todo Monte Carlo com programaÃ§Ã£o paralela atravÃ©s do MPI (Message Passing Interface). O mÃ©todo Monte Carlo Ã© uma tÃ©cnica estatÃ­stica que usa nÃºmeros aleatÃ³rios para resolver problemas matemÃ¡ticos, sendo ideal para demonstrar conceitos de computaÃ§Ã£o paralela.

### MÃ©todo Monte Carlo para CÃ¡lculo de Pi

O mÃ©todo baseia-se na relaÃ§Ã£o geomÃ©trica entre um cÃ­rculo inscrito em um quadrado:

1. **PrincÃ­pio**: Consideramos um cÃ­rculo de raio 1 inscrito em um quadrado de lado 2
2. **Ãrea do cÃ­rculo**: Ï€ Ã— rÂ² = Ï€ Ã— 1Â² = Ï€
3. **Ãrea do quadrado**: (2r)Â² = 4
4. **RazÃ£o**: Ï€/4 = (Ã¡rea do cÃ­rculo)/(Ã¡rea do quadrado)

**Algoritmo**:
- Gerar pontos aleatÃ³rios (x, y) no intervalo [-1, 1]
- Verificar se cada ponto estÃ¡ dentro do cÃ­rculo: xÂ² + yÂ² â‰¤ 1
- Calcular a razÃ£o: pontos_dentro_cÃ­rculo / total_pontos
- Estimar Ï€: 4 Ã— (pontos_dentro_cÃ­rculo / total_pontos)

## MPI: DistribuÃ­do vs Threads

### ğŸŒ **MPI Ã© para ComputaÃ§Ã£o DistribuÃ­da**
- **MPI usa PROCESSOS**, nÃ£o threads
- **Funciona em mÃºltiplas mÃ¡quinas** conectadas por rede
- **MemÃ³ria distribuÃ­da**: cada processo tem sua prÃ³pria memÃ³ria
- **ComunicaÃ§Ã£o via mensagens**: processos se comunicam enviando dados
- **EscalÃ¡vel para clusters e supercomputadores**

### ğŸ”§ **Verificar Slots DisponÃ­veis**

```bash
# Ver mapeamento de processos
mpirun --display-map --np 8 hostname

# Ver alocaÃ§Ã£o de recursos  
mpirun --display-allocation --np 4 hostname

# NÃºmero de cores do sistema
sysctl -n hw.ncpu  # macOS
nproc              # Linux
```

### ğŸŒ **ExecuÃ§Ã£o DistribuÃ­da**

#### Arquivo de Hosts (hostfile)
```bash
# Criar hostfile para mÃºltiplas mÃ¡quinas
echo "server1 slots=8" > hostfile
echo "server2 slots=4" >> hostfile
echo "192.168.1.10 slots=16" >> hostfile

# Executar distribuÃ­do
mpirun -np 28 --hostfile hostfile ./monte_carlo_pi 10000000
```

#### ConfiguraÃ§Ã£o SSH
```bash
# Configurar acesso sem senha
ssh-keygen -t rsa
ssh-copy-id user@server1
ssh-copy-id user@server2
```

## Plano da SoluÃ§Ã£o com MPI

### Arquitectura Paralela

**EstratÃ©gia de ParalelizaÃ§Ã£o**:
- **Processo Master (rank 0)**: Coordena a execuÃ§Ã£o, coleta resultados e calcula Pi final
- **Processos Worker (rank > 0)**: Executam simulaÃ§Ãµes Monte Carlo independentes
- **DistribuiÃ§Ã£o do trabalho**: Cada processo gera N/P pontos (onde N = total de pontos, P = nÃºmero de processos)

### Funcionalidades MPI Utilizadas

#### 1. **InicializaÃ§Ã£o e FinalizaÃ§Ã£o**
```c
MPI_Init(&argc, &argv)     // Inicializar ambiente MPI
MPI_Finalize()             // Finalizar ambiente MPI
```

#### 2. **IdentificaÃ§Ã£o de Processos**
```c
MPI_Comm_rank(MPI_COMM_WORLD, &rank)  // Identifica o rank do processo
MPI_Comm_size(MPI_COMM_WORLD, &size)  // ObtÃ©m nÃºmero total de processos
```

#### 3. **ComunicaÃ§Ã£o Point-to-Point**
```c
MPI_Send()  // Processos worker enviam resultados para master
MPI_Recv()  // Processo master recebe resultados dos workers
```

#### 4. **OperaÃ§Ãµes Coletivas**
```c
MPI_Reduce()     // ReduÃ§Ã£o para somar todos os pontos dentro do cÃ­rculo
MPI_Bcast()      // Broadcast do nÃºmero de pontos por processo (opcional)
```

#### 5. **MediÃ§Ã£o de Tempo**
```c
MPI_Wtime()  // Medir tempo de execuÃ§Ã£o para anÃ¡lise de performance
```

### Estrutura do Programa

#### Arquivos do Projeto

```
/
â”œâ”€â”€ README.md              # Este arquivo
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ monte_carlo_pi.c   # Programa principal MPI
â”‚   â””â”€â”€ utils.h            # FunÃ§Ãµes utilitÃ¡rias
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ compile.sh         # Script de compilaÃ§Ã£o
â”‚   â””â”€â”€ run.sh            # Script de execuÃ§Ã£o
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ hostfile           # Exemplo de configuraÃ§Ã£o distribuÃ­da
â”‚   â””â”€â”€ hostfile_simple    # ConfiguraÃ§Ã£o local simples
â”œâ”€â”€ results/
â”‚   â””â”€â”€ performance.txt    # Resultados de performance
â””â”€â”€ docs/
    â”œâ”€â”€ mpi_analysis.md    # AnÃ¡lise das funcionalidades MPI
    â””â”€â”€ mpi_distributed.md # Guia de execuÃ§Ã£o distribuÃ­da
```

### Algoritmo Detalhado

#### Processo Master (rank 0):
1. Inicializar MPI
2. Determinar nÃºmero de pontos por processo
3. Executar simulaÃ§Ã£o Monte Carlo local
4. Receber resultados dos processos worker
5. Calcular Pi final e tempo de execuÃ§Ã£o
6. Exibir resultados e estatÃ­sticas

#### Processos Worker (rank > 0):
1. Inicializar MPI
2. Receber nÃºmero de pontos para processar
3. Executar simulaÃ§Ã£o Monte Carlo local
4. Enviar resultado para processo master

### AnÃ¡lise de Performance

**MÃ©tricas a serem medidas**:
- Tempo de execuÃ§Ã£o sequencial vs paralelo
- Speedup: T_sequencial / T_paralelo
- EficiÃªncia: Speedup / nÃºmero_de_processos
- Escalabilidade com diferentes nÃºmeros de processos

**Testes Planejados**:
- ExecuÃ§Ã£o com 1, 2, 4, 8 processos
- Diferentes nÃºmeros de pontos (10â¶, 10â·, 10â¸)
- AnÃ¡lise da precisÃ£o vs. performance

### Funcionalidades MPI em Detalhes

#### 1. **MPI_Init e MPI_Finalize**
- **PropÃ³sito**: InicializaÃ§Ã£o e finalizaÃ§Ã£o do ambiente MPI
- **Uso**: ObrigatÃ³rio no inÃ­cio e fim de qualquer programa MPI

#### 2. **MPI_Comm_rank e MPI_Comm_size**
- **PropÃ³sito**: IdentificaÃ§Ã£o de processos e conhecimento do ambiente
- **Uso**: Determinar papel de cada processo (master/worker)

#### 3. **MPI_Send e MPI_Recv**
- **PropÃ³sito**: ComunicaÃ§Ã£o ponto-a-ponto bloqueante
- **Uso**: Workers enviam contagem de pontos para master

#### 4. **MPI_Reduce**
- **PropÃ³sito**: OperaÃ§Ã£o coletiva de reduÃ§Ã£o (soma)
- **Uso**: Somar todos os pontos dentro do cÃ­rculo de todos os processos
- **Vantagem**: Mais eficiente que mÃºltiplos Send/Recv

#### 5. **MPI_Wtime**
- **PropÃ³sito**: MediÃ§Ã£o precisa de tempo
- **Uso**: AnÃ¡lise de performance e benchmarking

### CompilaÃ§Ã£o e ExecuÃ§Ã£o

```bash
# CompilaÃ§Ã£o
mpicc -o monte_carlo_pi src/monte_carlo_pi.c -lm

# ExecuÃ§Ã£o local
mpirun -np 4 ./monte_carlo_pi 1000000

# ExecuÃ§Ã£o distribuÃ­da
mpirun -np 16 --hostfile hostfile ./monte_carlo_pi 10000000

# ExecuÃ§Ã£o com diferentes configuraÃ§Ãµes
mpirun -np 2 ./monte_carlo_pi 10000000
mpirun -np 8 ./monte_carlo_pi 100000000
```

### Usando o Makefile

```bash
# Compilar
make compile

# Testes rÃ¡pidos
make test              # Teste bÃ¡sico
make test-scaling      # Teste de escalabilidade  
make test-precision    # Teste de precisÃ£o

# Verificar sistema
make check-mpi         # Verificar se MPI estÃ¡ instalado
make mpi-info          # InformaÃ§Ãµes do sistema MPI

# Limpeza
make clean            # Remover arquivos gerados
```

### Resultados Esperados

**SaÃ­da do Programa**:
```
Calculando Pi usando Monte Carlo com MPI
NÃºmero de processos: 4
Pontos por processo: 250000
Total de pontos: 1000000

Processo 0: 196350 pontos dentro do cÃ­rculo
Processo 1: 196428 pontos dentro do cÃ­rculo  
Processo 2: 196502 pontos dentro do cÃ­rculo
Processo 3: 196381 pontos dentro do cÃ­rculo

Total de pontos dentro do cÃ­rculo: 785661
Pi estimado: 3.142644
Pi real: 3.141593
Erro: 0.033%
Tempo de execuÃ§Ã£o: 0.025 segundos
Speedup: 3.2x
EficiÃªncia: 80%
```

### Vantagens da SoluÃ§Ã£o MPI

1. **Escalabilidade**: Pode ser executado em mÃºltiplas mÃ¡quinas
2. **Flexibilidade**: Funciona em clusters e supercomputadores
3. **Portabilidade**: PadrÃ£o MPI Ã© amplamente suportado
4. **Performance**: DistribuiÃ§Ã£o eficiente do trabalho computacional

### Conceitos de MPI Demonstrados

- **SPMD** (Single Program, Multiple Data): Mesmo programa, dados diferentes
- **DecomposiÃ§Ã£o de domÃ­nio**: DivisÃ£o do problema em partes independentes
- **ComunicaÃ§Ã£o coletiva**: Uso eficiente de operaÃ§Ãµes MPI
- **SincronizaÃ§Ã£o**: CoordenaÃ§Ã£o entre processos
- **Load balancing**: DistribuiÃ§Ã£o equilibrada do trabalho

### ExtensÃµes Futuras

1. **MPI_Scatter/MPI_Gather**: Para distribuiÃ§Ã£o mais sofisticada
2. **MPI_Isend/MPI_Irecv**: ComunicaÃ§Ã£o nÃ£o-bloqueante
3. **MÃºltiplos communicators**: Para hierarquias de processos
4. **MPI-IO**: Para escrita paralela de resultados

Este projeto fornece uma base sÃ³lida para compreender os conceitos fundamentais do MPI e sua aplicaÃ§Ã£o em problemas de computaÃ§Ã£o cientÃ­fica.

# Utilizando Docker Swarm com MPI DistribuÃ­do

Este projeto demonstra como executar um programa paralelo com MPI (Message Passing Interface) utilizando mÃºltiplos containers Docker que atuam como nÃ³s de um cluster.

No exemplo, usaremos 16 nÃ³s Docker em um cluster Swarm, mas esse nÃºmero pode ser ajustado conforme a necessidade.

Cada nÃ³ roda uma imagem Docker configurada com:

- OpenSSH Server para permitir acesso remoto via SSH.

- MPICH (implementaÃ§Ã£o do MPI).

- UsuÃ¡rio `mpiuser` com autenticaÃ§Ã£o via chave SSH, permitindo comunicaÃ§Ã£o sem senha entre os nÃ³s.

- Rede Docker do tipo overlay `mpi-net` para comunicaÃ§Ã£o entre containers.

## 1. Build da imagem base

No **diretÃ³rio raiz do projeto** (onde estÃ¡ a pasta docker e o Dockerfile), rode o comando:
```bash
docker build -t mpi-node:latest -f docker/Dockerfile .
```

Isso cria a imagem Docker com todas as dependÃªncias para MPI e SSH.

## 2. Inicializar o Docker Swarm

Se vocÃª ainda nÃ£o iniciou o Swarm, rode o comando: `docker swarm init`

## 3. Escolher o nÃºmero de rÃ©plicas

VocÃª pode modificar o `docker-compose.yml` no atributo `replicas` para escolher o nÃºmero de nÃ³s que serÃ£o criados. O exemplo aqui usarÃ¡ 16, como estÃ¡ no arquivo, caso modifique, modifique tambÃ©m onde aparecer o **16** pelo nÃºmero escolhido por vocÃª.

## 4. Deploy da stack no Swarm

Rode o comando abaixo **dentro do diretÃ³rio** `docker` para criar a stack com o compose: 

```bash
docker stack deploy -c docker-compose.yml mpi_stack
```

VocÃª verÃ¡ os serviÃ§os sendo criados e rÃ©plicas iniciadas.

## 5. Verificar status dos serviÃ§os e containers

Para checar os serviÃ§os: 

```bash
docker service ls
```

Para ver os containers criados: 
```bash
docker service ps mpi_stack_mpi-node
```

## 6. Testar conectividade e SSH entre os nÃ³s

Para facilitar, acesse o terminal de um dos containers (por exemplo, o nÃ³ 1):

```bash
docker exec -u mpiuser -it $(docker ps --format "{{.Names}}" | Where-Object { $_ -match "^mpi_stack_mpi-node\.1\." }) bash
```

Agora, dentro do container como mpiuser, faÃ§a os testes:

### 6.1. Verificar conectividade via ping

```bash
for host in $(seq 1 16 | sed 's/^/mpi-node-/'); do
  echo "Pingando $host..."
  ping -c 1 -W 1 $host && echo "Ping OK" || echo "Ping falhou"
done
```
Isso confirma que todos os nÃ³s estÃ£o acessÃ­veis pela rede Docker.

### 6.2. Testar conexÃ£o SSH
O comando abaixo tenta conectar via SSH a partir de mpi-node1 e evita o prompt interativo da primeira conexÃ£o:

```bash
for host in $(seq 1 16 | sed 's/^/mpi-node-/'); do
  echo "Testando SSH em $host..."
  ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 mpiuser@$host "echo 'ConexÃ£o OK em $host'" || echo "Falha SSH em $host"
done
```

Com isso os hosts tambÃ©m sÃ£o adicionados ao `known_hosts`, permitindo executar o MPI distribuÃ­do.

## 7. Executar o MPI distribuÃ­do
Uma vez que os testes de rede e SSH estejam funcionando:

```bash
mpirun -np 16 --host $(seq 1 16 | sed 's/^/mpi-node-/; s/$/,/' | tr -d '\n' | sed 's/,$//') /home/mpiuser/monte_carlo_pi 1000000000
```
Esse comando executa o programa monte_carlo_pi de forma distribuÃ­da nos 16 nÃ³s.

# ğŸŒ¥ï¸ **Cluster HÃ­brido AWS (AvanÃ§ado)**

Pode executar seu programa distribuÃ­do entre **sua mÃ¡quina local e 1 instÃ¢ncia EC2** na nuvem!

### ConfiguraÃ§Ã£o RÃ¡pida
```bash
# 1. Instalar AWS CLI e configurar
pip install awscli
aws configure

# 2. Configurar cluster automaticamente
make aws-setup

# 3. Executar distribuÃ­do (10 processos: 8 local + 2 EC2)
make aws-run
```

### Arquitetura do Cluster
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Sua MÃ¡quina   â”‚    â”‚     EC2-1       â”‚
â”‚   (macOS)       â”‚    â”‚   (Ubuntu)      â”‚
â”‚   8 slots       â”‚â—„â”€â”€â–ºâ”‚   2 slots       â”‚
â”‚                 â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Custo Estimado
- **1 instÃ¢ncia t3.medium**: ~$0.04/hora
- **Teste de 1 dia**: ~$1.00
- **Muito econÃ´mico** para demonstraÃ§Ã£o de MPI distribuÃ­do

### Comandos Ãšteis
```bash
make aws-check      # Verificar dependÃªncias
make aws-setup      # Configurar cluster completo
make aws-test       # Testar conectividade
make aws-run        # Executar programa distribuÃ­do
make aws-cleanup    # Terminar instÃ¢ncia EC2
```

> ğŸ“– **Manual completo**: `docs/aws_cluster_setup.md` 