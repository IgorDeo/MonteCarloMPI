# Projeto MPI: C√°lculo de Pi usando Monte Carlo

## Descri√ß√£o do Problema

Este projeto implementa o c√°lculo do valor de Pi (œÄ) utilizando o m√©todo Monte Carlo com programa√ß√£o paralela atrav√©s do MPI (Message Passing Interface). O m√©todo Monte Carlo √© uma t√©cnica estat√≠stica que usa n√∫meros aleat√≥rios para resolver problemas matem√°ticos, sendo ideal para demonstrar conceitos de computa√ß√£o paralela.

### M√©todo Monte Carlo para C√°lculo de Pi

O m√©todo baseia-se na rela√ß√£o geom√©trica entre um c√≠rculo inscrito em um quadrado:

1. **Princ√≠pio**: Consideramos um c√≠rculo de raio 1 inscrito em um quadrado de lado 2
2. **√Årea do c√≠rculo**: œÄ √ó r¬≤ = œÄ √ó 1¬≤ = œÄ
3. **√Årea do quadrado**: (2r)¬≤ = 4
4. **Raz√£o**: œÄ/4 = (√°rea do c√≠rculo)/(√°rea do quadrado)

**Algoritmo**:
- Gerar pontos aleat√≥rios (x, y) no intervalo [-1, 1]
- Verificar se cada ponto est√° dentro do c√≠rculo: x¬≤ + y¬≤ ‚â§ 1
- Calcular a raz√£o: pontos_dentro_c√≠rculo / total_pontos
- Estimar œÄ: 4 √ó (pontos_dentro_c√≠rculo / total_pontos)

## MPI: Distribu√≠do vs Threads

### üåê **MPI √© para Computa√ß√£o Distribu√≠da**
- **MPI usa PROCESSOS**, n√£o threads
- **Funciona em m√∫ltiplas m√°quinas** conectadas por rede
- **Mem√≥ria distribu√≠da**: cada processo tem sua pr√≥pria mem√≥ria
- **Comunica√ß√£o via mensagens**: processos se comunicam enviando dados
- **Escal√°vel para clusters e supercomputadores**

### üîß **Verificar Slots Dispon√≠veis**

```bash
# Ver mapeamento de processos
mpirun --display-map --np 8 hostname

# Ver aloca√ß√£o de recursos  
mpirun --display-allocation --np 4 hostname

# N√∫mero de cores do sistema
sysctl -n hw.ncpu  # macOS
nproc              # Linux
```

### üåê **Execu√ß√£o Distribu√≠da**

#### Arquivo de Hosts (hostfile)
```bash
# Criar hostfile para m√∫ltiplas m√°quinas
echo "server1 slots=8" > hostfile
echo "server2 slots=4" >> hostfile
echo "192.168.1.10 slots=16" >> hostfile

# Executar distribu√≠do
mpirun -np 28 --hostfile hostfile ./monte_carlo_pi 10000000
```

#### Configura√ß√£o SSH
```bash
# Configurar acesso sem senha
ssh-keygen -t rsa
ssh-copy-id user@server1
ssh-copy-id user@server2
```

## Plano da Solu√ß√£o com MPI

### Arquitectura Paralela

**Estrat√©gia de Paraleliza√ß√£o**:
- **Processo Master (rank 0)**: Coordena a execu√ß√£o, coleta resultados e calcula Pi final
- **Processos Worker (rank > 0)**: Executam simula√ß√µes Monte Carlo independentes
- **Distribui√ß√£o do trabalho**: Cada processo gera N/P pontos (onde N = total de pontos, P = n√∫mero de processos)

### Funcionalidades MPI Utilizadas

#### 1. **Inicializa√ß√£o e Finaliza√ß√£o**
```c
MPI_Init(&argc, &argv)     // Inicializar ambiente MPI
MPI_Finalize()             // Finalizar ambiente MPI
```

#### 2. **Identifica√ß√£o de Processos**
```c
MPI_Comm_rank(MPI_COMM_WORLD, &rank)  // Identifica o rank do processo
MPI_Comm_size(MPI_COMM_WORLD, &size)  // Obt√©m n√∫mero total de processos
```

#### 3. **Comunica√ß√£o Point-to-Point**
```c
MPI_Send()  // Processos worker enviam resultados para master
MPI_Recv()  // Processo master recebe resultados dos workers
```

#### 4. **Opera√ß√µes Coletivas**
```c
MPI_Reduce()     // Redu√ß√£o para somar todos os pontos dentro do c√≠rculo
MPI_Bcast()      // Broadcast do n√∫mero de pontos por processo (opcional)
```

#### 5. **Medi√ß√£o de Tempo**
```c
MPI_Wtime()  // Medir tempo de execu√ß√£o para an√°lise de performance
```

### Estrutura do Programa

#### Arquivos do Projeto

```
/
‚îú‚îÄ‚îÄ README.md              # Este arquivo
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ monte_carlo_pi.c   # Programa principal MPI
‚îÇ   ‚îî‚îÄ‚îÄ utils.h            # Fun√ß√µes utilit√°rias
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ compile.sh         # Script de compila√ß√£o
‚îÇ   ‚îî‚îÄ‚îÄ run.sh            # Script de execu√ß√£o
‚îú‚îÄ‚îÄ examples/
‚îÇ   ‚îú‚îÄ‚îÄ hostfile           # Exemplo de configura√ß√£o distribu√≠da
‚îÇ   ‚îî‚îÄ‚îÄ hostfile_simple    # Configura√ß√£o local simples
‚îú‚îÄ‚îÄ results/
‚îÇ   ‚îî‚îÄ‚îÄ performance.txt    # Resultados de performance
‚îî‚îÄ‚îÄ docs/
    ‚îú‚îÄ‚îÄ mpi_analysis.md    # An√°lise das funcionalidades MPI
    ‚îî‚îÄ‚îÄ mpi_distributed.md # Guia de execu√ß√£o distribu√≠da
```

### Algoritmo Detalhado

#### Processo Master (rank 0):
1. Inicializar MPI
2. Determinar n√∫mero de pontos por processo
3. Executar simula√ß√£o Monte Carlo local
4. Receber resultados dos processos worker
5. Calcular Pi final e tempo de execu√ß√£o
6. Exibir resultados e estat√≠sticas

#### Processos Worker (rank > 0):
1. Inicializar MPI
2. Receber n√∫mero de pontos para processar
3. Executar simula√ß√£o Monte Carlo local
4. Enviar resultado para processo master

### An√°lise de Performance

**M√©tricas a serem medidas**:
- Tempo de execu√ß√£o sequencial vs paralelo
- Speedup: T_sequencial / T_paralelo
- Efici√™ncia: Speedup / n√∫mero_de_processos
- Escalabilidade com diferentes n√∫meros de processos

**Testes Planejados**:
- Execu√ß√£o com 1, 2, 4, 8 processos
- Diferentes n√∫meros de pontos (10‚Å∂, 10‚Å∑, 10‚Å∏)
- An√°lise da precis√£o vs. performance

### Funcionalidades MPI em Detalhes

#### 1. **MPI_Init e MPI_Finalize**
- **Prop√≥sito**: Inicializa√ß√£o e finaliza√ß√£o do ambiente MPI
- **Uso**: Obrigat√≥rio no in√≠cio e fim de qualquer programa MPI

#### 2. **MPI_Comm_rank e MPI_Comm_size**
- **Prop√≥sito**: Identifica√ß√£o de processos e conhecimento do ambiente
- **Uso**: Determinar papel de cada processo (master/worker)

#### 3. **MPI_Send e MPI_Recv**
- **Prop√≥sito**: Comunica√ß√£o ponto-a-ponto bloqueante
- **Uso**: Workers enviam contagem de pontos para master

#### 4. **MPI_Reduce**
- **Prop√≥sito**: Opera√ß√£o coletiva de redu√ß√£o (soma)
- **Uso**: Somar todos os pontos dentro do c√≠rculo de todos os processos
- **Vantagem**: Mais eficiente que m√∫ltiplos Send/Recv

#### 5. **MPI_Wtime**
- **Prop√≥sito**: Medi√ß√£o precisa de tempo
- **Uso**: An√°lise de performance e benchmarking

### Compila√ß√£o e Execu√ß√£o

```bash
# Compila√ß√£o
mpicc -o monte_carlo_pi src/monte_carlo_pi.c -lm

# Execu√ß√£o local
mpirun -np 4 ./monte_carlo_pi 1000000

# Execu√ß√£o distribu√≠da
mpirun -np 16 --hostfile hostfile ./monte_carlo_pi 10000000

# Execu√ß√£o com diferentes configura√ß√µes
mpirun -np 2 ./monte_carlo_pi 10000000
mpirun -np 8 ./monte_carlo_pi 100000000
```

### Usando o Makefile

```bash
# Compilar
make compile

# Testes r√°pidos
make test              # Teste b√°sico
make test-scaling      # Teste de escalabilidade  
make test-precision    # Teste de precis√£o

# Verificar sistema
make check-mpi         # Verificar se MPI est√° instalado
make mpi-info          # Informa√ß√µes do sistema MPI

# Limpeza
make clean            # Remover arquivos gerados
```

### Resultados Esperados

**Sa√≠da do Programa**:
```
Calculando Pi usando Monte Carlo com MPI
N√∫mero de processos: 4
Pontos por processo: 250000
Total de pontos: 1000000

Processo 0: 196350 pontos dentro do c√≠rculo
Processo 1: 196428 pontos dentro do c√≠rculo  
Processo 2: 196502 pontos dentro do c√≠rculo
Processo 3: 196381 pontos dentro do c√≠rculo

Total de pontos dentro do c√≠rculo: 785661
Pi estimado: 3.142644
Pi real: 3.141593
Erro: 0.033%
Tempo de execu√ß√£o: 0.025 segundos
Speedup: 3.2x
Efici√™ncia: 80%
```

### Vantagens da Solu√ß√£o MPI

1. **Escalabilidade**: Pode ser executado em m√∫ltiplas m√°quinas
2. **Flexibilidade**: Funciona em clusters e supercomputadores
3. **Portabilidade**: Padr√£o MPI √© amplamente suportado
4. **Performance**: Distribui√ß√£o eficiente do trabalho computacional

### Conceitos de MPI Demonstrados

- **SPMD** (Single Program, Multiple Data): Mesmo programa, dados diferentes
- **Decomposi√ß√£o de dom√≠nio**: Divis√£o do problema em partes independentes
- **Comunica√ß√£o coletiva**: Uso eficiente de opera√ß√µes MPI
- **Sincroniza√ß√£o**: Coordena√ß√£o entre processos
- **Load balancing**: Distribui√ß√£o equilibrada do trabalho

### Extens√µes Futuras

1. **MPI_Scatter/MPI_Gather**: Para distribui√ß√£o mais sofisticada
2. **MPI_Isend/MPI_Irecv**: Comunica√ß√£o n√£o-bloqueante
3. **M√∫ltiplos communicators**: Para hierarquias de processos
4. **MPI-IO**: Para escrita paralela de resultados

Este projeto fornece uma base s√≥lida para compreender os conceitos fundamentais do MPI e sua aplica√ß√£o em problemas de computa√ß√£o cient√≠fica.

# Utilizando Docker Swarm com MPI Distribu√≠do

Este projeto demonstra como executar um programa paralelo com MPI (Message Passing Interface) utilizando m√∫ltiplos containers Docker que atuam como n√≥s de um cluster.

No exemplo, usaremos 16 n√≥s Docker em um cluster Swarm, mas esse n√∫mero pode ser ajustado conforme a necessidade.

Cada n√≥ roda uma imagem Docker configurada com:

- OpenSSH Server para permitir acesso remoto via SSH.

- MPICH (implementa√ß√£o do MPI).

- Usu√°rio `mpiuser` com autentica√ß√£o via chave SSH, permitindo comunica√ß√£o sem senha entre os n√≥s.

- Rede Docker do tipo overlay `mpi-net` para comunica√ß√£o entre containers.

## 1. Build da imagem base

No **diret√≥rio raiz do projeto** (onde est√° a pasta docker e o Dockerfile), rode o comando:
```bash
docker build -t mpi-node:latest -f docker/Dockerfile .
```

Isso cria a imagem Docker com todas as depend√™ncias para MPI e SSH.

## 2. Inicializar o Docker Swarm

Se voc√™ ainda n√£o iniciou o Swarm, rode o comando: `docker swarm init`

## 3. Escolher o n√∫mero de r√©plicas

Voc√™ pode modificar o `docker-compose.yml` no atributo `replicas` para escolher o n√∫mero de n√≥s que ser√£o criados. O exemplo aqui usar√° 16, como est√° no arquivo, caso modifique, modifique tamb√©m onde aparecer o **16** pelo n√∫mero escolhido por voc√™.

## 4. Deploy da stack no Swarm

Rode o comando abaixo **dentro do diret√≥rio** `docker` para criar a stack com o compose: 

```bash
docker stack deploy -c docker-compose.yml mpi_stack
```

Voc√™ ver√° os servi√ßos sendo criados e r√©plicas iniciadas.

## 5. Verificar status dos servi√ßos e containers

Para checar os servi√ßos: 

```bash
docker service ls
```

Para ver os containers criados: 
```bash
docker service ps mpi_stack_mpi-node
```

## 6. Testar conectividade e SSH entre os n√≥s

Para facilitar, acesse o terminal de um dos containers (por exemplo, o n√≥ 1):

```bash
docker exec -u mpiuser -it $(docker ps --format "{{.Names}}" | Where-Object { $_ -match "^mpi_stack_mpi-node\.1\." }) bash
```

Agora, dentro do container como mpiuser, fa√ßa os testes:

### 6.1. Verificar conectividade via ping

```bash
for host in $(seq 1 16 | sed 's/^/mpi-node-/'); do
  echo "Pingando $host..."
  ping -c 1 -W 1 $host && echo "Ping OK" || echo "Ping falhou"
done
```
Isso confirma que todos os n√≥s est√£o acess√≠veis pela rede Docker.

### 6.2. Testar conex√£o SSH
O comando abaixo tenta conectar via SSH a partir de mpi-node1 e evita o prompt interativo da primeira conex√£o:

```bash
for host in $(seq 1 16 | sed 's/^/mpi-node-/'); do
  echo "Testando SSH em $host..."
  ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 mpiuser@$host "echo 'Conex√£o OK em $host'" || echo "Falha SSH em $host"
done
```

Com isso os hosts tamb√©m s√£o adicionados ao `known_hosts`, permitindo executar o MPI distribu√≠do.

## 7. Executar o MPI distribu√≠do
Uma vez que os testes de rede e SSH estejam funcionando:

```bash
mpirun -np 16 --host $(seq 1 16 | sed 's/^/mpi-node-/; s/$/,/' | tr -d '\n' | sed 's/,$//') /home/mpiuser/monte_carlo_pi 1000000000
```
Esse comando executa o programa monte_carlo_pi de forma distribu√≠da nos 16 n√≥s.

# üå•Ô∏è **Cluster H√≠brido AWS (Avan√ßado)**

Pode executar seu programa distribu√≠do entre **sua m√°quina local e 1 inst√¢ncia EC2** na nuvem!

### Configura√ß√£o R√°pida
```bash
# 1. Instalar AWS CLI e configurar
pip install awscli
aws configure

# 2. Configurar cluster automaticamente
make aws-setup

# 3. Executar distribu√≠do (10 processos: 8 local + 2 EC2)
make aws-run
```

### Arquitetura do Cluster
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Sua M√°quina   ‚îÇ    ‚îÇ     EC2-1       ‚îÇ
‚îÇ   (macOS)       ‚îÇ    ‚îÇ   (Ubuntu)      ‚îÇ
‚îÇ   8 slots       ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ   2 slots       ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Custo Estimado
- **1 inst√¢ncia t3.medium**: ~$0.04/hora
- **Teste de 1 dia**: ~$1.00
- **Muito econ√¥mico** para demonstra√ß√£o de MPI distribu√≠do

### Comandos √öteis
```bash
make aws-check      # Verificar depend√™ncias
make aws-setup      # Configurar cluster completo
make aws-test       # Testar conectividade
make aws-run        # Executar programa distribu√≠do
make aws-cleanup    # Terminar inst√¢ncia EC2
```

> üìñ **Manual completo**: `docs/aws_cluster_setup.md` 