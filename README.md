# Projeto MPI: C√°lculo de Pi usando Monte Carlo

## üìã Descri√ß√£o do Projeto

Este projeto implementa o c√°lculo do valor de Pi (œÄ) utilizando o m√©todo Monte Carlo com programa√ß√£o paralela atrav√©s do MPI (Message Passing Interface). O projeto oferece duas formas de execu√ß√£o:

1. **Execu√ß√£o Local**: Usando MPI instalado diretamente no sistema
2. **Execu√ß√£o Distribu√≠da**: Usando containers Docker para simular um cluster

### üéØ M√©todo Monte Carlo para C√°lculo de Pi

O m√©todo baseia-se na rela√ß√£o geom√©trica entre um c√≠rculo inscrito em um quadrado:

1. **Princ√≠pio**: Consideramos um c√≠rculo de raio 1 inscrito em um quadrado de lado 2
2. **√Årea do c√≠rculo**: œÄ √ó r¬≤ = œÄ √ó 1¬≤ = œÄ  
3. **√Årea do quadrado**: (2r)¬≤ = 4
4. **Raz√£o**: œÄ/4 = (√°rea do c√≠rculo)/(√°rea do quadrado)

**Algoritmo**:
- Gerar pontos aleat√≥rios (x, y) no intervalo [-1, 1]
- Verificar se cada ponto est√° dentro do c√≠rculo: x¬≤ + y¬≤ ‚â§ 1
- Calcular a raz√£o: pontos_dentro_c√≠rculo / total_pontos
- Estimar œÄ: 4 √ó (pontos_dentro_c√≠rculo / total_pontos)

## üèóÔ∏è Estrutura do Projeto

```
/
‚îú‚îÄ‚îÄ README.md              # Documenta√ß√£o completa
‚îú‚îÄ‚îÄ Makefile              # Comandos automatizados
‚îú‚îÄ‚îÄ Dockerfile        # Imagem Alpine otimizada (36.3MB)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ monte_carlo_pi.c   # Programa principal MPI
‚îÇ   ‚îî‚îÄ‚îÄ utils.h            # Fun√ß√µes utilit√°rias
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ compile.sh         # Script de compila√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ run.sh            # Script de execu√ß√£o local
‚îÇ   ‚îî‚îÄ‚îÄ run_docker.sh     # Script para containers Docker
```

## üöÄ Execu√ß√£o R√°pida

### Op√ß√£o 1: Execu√ß√£o Local (MPI no Sistema)

```bash
# Compilar e testar
make test

# Testes com diferentes configura√ß√µes
make test-scaling      # Teste de escalabilidade
make test-precision    # Teste de precis√£o
```

### Op√ß√£o 2: Execu√ß√£o com Docker (Recomendado)

```bash
# 1. Deploy dos containers
make docker-deploy

# 2. Executar c√°lculo distribu√≠do
make docker-run

# 3. Limpar quando terminar
make docker-cleanup
```

## üê≥ Execu√ß√£o com Docker Containers

### Vantagens da Solu√ß√£o Docker

- ‚úÖ **Imagem Ultra-Otimizada**: Alpine Linux (36.3MB - 94% menor que Ubuntu)
- ‚úÖ **Isolamento Completo**: Cada processo roda em container separado
- ‚úÖ **Portabilidade Total**: Funciona em qualquer sistema com Docker
- ‚úÖ **Setup Autom√°tico**: Um comando faz todo o deploy
- ‚úÖ **Escalabilidade F√°cil**: Ajuste o n√∫mero de containers dinamicamente

### Comandos Docker Dispon√≠veis

| Comando | Descri√ß√£o |
|---------|-----------|
| `make docker-build` | Construir imagem otimizada |
| `make docker-deploy` | Deploy dos containers MPI |
| `make docker-run` | Executar programa distribu√≠do |
| `make docker-status` | Ver status dos containers |
| `make docker-scale` | Escalar n√∫mero de containers |
| `make docker-menu` | Menu interativo completo |
| `make docker-cleanup` | Limpar containers |

### Menu Interativo Docker

Para uma experi√™ncia mais amig√°vel:

```bash
make docker-menu
```

Isso abrir√° um menu completo:

```
===============================================
    MPI Distribu√≠do com Docker Containers
===============================================
1. Verificar depend√™ncias
2. Construir imagem otimizada  
3. Deploy dos containers
4. Verificar status dos containers
5. Testar conectividade
6. Executar programa MPI
7. Escalar containers
8. Ver logs
9. Limpar containers
0. Sair
===============================================
```

### Execu√ß√£o Personalizada

```bash
# Deploy com n√∫mero espec√≠fico de containers
./scripts/run_docker.sh deploy 4

# Executar com par√¢metros personalizados
./scripts/run_docker.sh run 8 1000000    # 8 processos, 1M pontos
./scripts/run_docker.sh run 16 50000000  # 16 processos, 50M pontos

# Escalar para mais containers
./scripts/run_docker.sh scale 12
```

## üíª Execu√ß√£o Local (MPI Nativo)

### Instala√ß√£o das Depend√™ncias

**Ubuntu/Debian:**
```bash
make install-deps-ubuntu
```

**macOS:**
```bash
make install-deps-macos
```

**Verificar instala√ß√£o:**
```bash
make check-mpi
make mpi-info
```

### Comandos de Execu√ß√£o Local

```bash
# Compila√ß√£o
make compile

# Testes b√°sicos
make test              # Teste r√°pido (4 processos, 100K pontos)
make test-scaling      # Teste de escalabilidade (1,2,4,8 processos)
make test-precision    # Teste de precis√£o (100K, 1M, 10M pontos)

# Execu√ß√£o manual
mpirun -np 4 ./build/monte_carlo_pi 1000000
mpirun -np 8 ./build/monte_carlo_pi 10000000
```

### Execu√ß√£o Distribu√≠da (M√∫ltiplas M√°quinas)

Para executar em m√∫ltiplas m√°quinas f√≠sicas:

```bash
# 1. Configurar SSH sem senha entre m√°quinas
ssh-keygen -t rsa
ssh-copy-id user@machine2
ssh-copy-id user@machine3

# 2. Criar arquivo hostfile
echo "machine1 slots=8" > hostfile
echo "machine2 slots=4" >> hostfile  
echo "machine3 slots=16" >> hostfile

# 3. Executar distribu√≠do
mpirun -np 28 --hostfile hostfile ./build/monte_carlo_pi 100000000
```

## üìä Resultados e Performance

### Exemplo de Sa√≠da

```
==========================================
Calculando Pi usando Monte Carlo com MPI
==========================================
N√∫mero de processos: 8
Total de pontos: 10000000
Pontos por processo: 1250000
------------------------------------------
Processo 0: 981976 pontos dentro do c√≠rculo (de 1250000 pontos)
Processo 1: 981311 pontos dentro do c√≠rculo (de 1250000 pontos)
Processo 2: 981706 pontos dentro do c√≠rculo (de 1250000 pontos)
Processo 3: 982444 pontos dentro do c√≠rculo (de 1250000 pontos)
Processo 4: 981970 pontos dentro do c√≠rculo (de 1250000 pontos)
Processo 5: 980772 pontos dentro do c√≠rculo (de 1250000 pontos)
Processo 6: 981788 pontos dentro do c√≠rculo (de 1250000 pontos)
Processo 7: 981803 pontos dentro do c√≠rculo (de 1250000 pontos)
------------------------------------------
RESULTADOS FINAIS:
Total de pontos dentro do c√≠rculo: 7853770
Pi estimado: 3.141508
Pi real: 3.141593
Erro absoluto: 0.000085
Erro percentual: 0.003%
Tempo de execu√ß√£o: 0.044737 segundos
------------------------------------------
```

## üî¨ Conceitos MPI Demonstrados

### Funcionalidades MPI Utilizadas

1. **Inicializa√ß√£o e Finaliza√ß√£o**
   ```c
   MPI_Init(&argc, &argv)     // Inicializar ambiente MPI
   MPI_Finalize()             // Finalizar ambiente MPI
   ```

2. **Identifica√ß√£o de Processos**
   ```c
   MPI_Comm_rank(MPI_COMM_WORLD, &rank)  // ID do processo
   MPI_Comm_size(MPI_COMM_WORLD, &size)  // Total de processos
   ```

3. **Sincroniza√ß√£o**
   ```c
   MPI_Barrier(MPI_COMM_WORLD)  // Sincronizar todos os processos
   ```

4. **Opera√ß√µes Coletivas**
   ```c
   MPI_Reduce()  // Redu√ß√£o para somar pontos dentro do c√≠rculo
   ```

5. **Medi√ß√£o de Tempo**
   ```c
   MPI_Wtime()  // Medi√ß√£o precisa de tempo para an√°lise
   ```

### Arquitetura Paralela

**Estrat√©gia SPMD (Single Program, Multiple Data):**
- **Todos os processos**: Executam o mesmo programa com dados diferentes
- **Distribui√ß√£o**: Cada processo calcula N/P pontos (N=total, P=processos)
- **Agrega√ß√£o**: MPI_Reduce soma todos os resultados locais

### Padr√µes de Paraleliza√ß√£o

1. **Decomposi√ß√£o de Dom√≠nio**: Problema dividido em partes independentes
2. **Load Balancing**: Trabalho distribu√≠do uniformemente
3. **Comunica√ß√£o Coletiva**: Uso eficiente de MPI_Reduce
4. **Sincroniza√ß√£o**: Coordena√ß√£o entre processos com MPI_Barrier

## üèóÔ∏è Detalhes T√©cnicos

### Imagem Docker Otimizada

A imagem Docker utiliza **Alpine Linux Multi-stage** para m√°xima otimiza√ß√£o:

```dockerfile
# Est√°gio 1: Build (descartado ap√≥s compila√ß√£o)
FROM alpine:3.19 AS builder
RUN apk add --no-cache openmpi-dev gcc g++ make libc-dev
COPY src/ /tmp/
RUN mpicc /tmp/monte_carlo_pi.c -o /tmp/monte_carlo_pi -lm

# Est√°gio 2: Runtime (imagem final)
FROM alpine:3.19  
RUN apk add --no-cache openmpi openssh-server openssh-client bash
COPY --from=builder /tmp/monte_carlo_pi /home/mpiuser/
# ... configura√ß√µes SSH e usu√°rio
```

**Vantagens:**
- **Tamanho**: 36.3MB (94% menor que Ubuntu)
- **Seguran√ßa**: Menor superf√≠cie de ataque
- **Performance**: Menos overhead, mais r√°pido
- **Portabilidade**: Funciona em qualquer Docker

### Algoritmo Detalhado

#### Algoritmo SPMD (Single Program, Multiple Data):
1. **Todos os processos** executam o mesmo c√≥digo
2. **Inicializa√ß√£o**: MPI_Init, obter rank e size
3. **Distribui√ß√£o**: Cada processo calcula pontos_totais/num_processos
4. **Sincroniza√ß√£o**: MPI_Barrier para medi√ß√£o precisa de tempo
5. **Simula√ß√£o**: Cada processo executa Monte Carlo independentemente
6. **Agrega√ß√£o**: MPI_Reduce soma resultados de todos os processos
7. **Resultado**: Processo rank 0 calcula Pi final e exibe resultados
8. **Finaliza√ß√£o**: MPI_Finalize em todos os processos

### Gera√ß√£o de N√∫meros Aleat√≥rios

O programa utiliza gerador congruencial linear com:
- **Semente √∫nica por processo**: Baseada no rank MPI
- **Per√≠odo longo**: Evita repeti√ß√£o de sequ√™ncias
- **Distribui√ß√£o uniforme**: Garante cobertura adequada do espa√ßo

## üß™ Testes e Valida√ß√£o

### Su√≠te de Testes Automatizada

```bash
# Teste b√°sico de funcionamento
make test

# An√°lise de escalabilidade
make test-scaling
# Testa com 1, 2, 4, 8 processos para medir speedup

# An√°lise de precis√£o  
make test-precision
# Testa com 100K, 1M, 10M pontos para medir converg√™ncia
```

### Valida√ß√£o dos Resultados

**Crit√©rios de Valida√ß√£o:**
- Erro percentual < 1% para 1M+ pontos
- Speedup pr√≥ximo ao n√∫mero de processos
- Efici√™ncia > 80% at√© 8 processos
- Converg√™ncia para Pi com mais pontos

## üîß Solu√ß√£o de Problemas

### Problemas Comuns

**1. "mpicc not found"**
```bash
# Ubuntu/Debian
sudo apt-get install libopenmpi-dev openmpi-bin

# macOS  
brew install open-mpi
```

**2. "Docker n√£o encontrado"**
```bash
# Instalar Docker Desktop
# Verificar se est√° rodando
docker --version
make docker-check
```

**3. "Permission denied" nos scripts**
```bash
chmod +x scripts/*.sh
```

**4. Containers n√£o inicializam**
```bash
# Limpar ambiente
make docker-cleanup
docker system prune -f

# Tentar novamente
make docker-deploy
```

### Debugging

```bash
# Ver logs detalhados dos containers
make docker-logs

# Verificar status
make docker-status

# Testar conectividade
make docker-test

# Executar comandos dentro do container
docker exec -it mpi-node-1 bash
```

## üìà Extens√µes Futuras

### Melhorias Poss√≠veis

1. **MPI Avan√ßado**
   - MPI_Scatter/MPI_Gather para distribui√ß√£o mais sofisticada
   - MPI_Isend/MPI_Irecv para comunica√ß√£o n√£o-bloqueante
   - M√∫ltiplos communicators para hierarquias

2. **Algoritmos**
   - Outros m√©todos Monte Carlo (integra√ß√£o, otimiza√ß√£o)
   - Algoritmos determin√≠sticos para compara√ß√£o
   - An√°lise estat√≠stica mais robusta

3. **Infraestrutura**
   - Kubernetes para orquestra√ß√£o em produ√ß√£o
   - Monitoramento com Prometheus/Grafana
   - CI/CD para testes automatizados

4. **Interface**
   - Web dashboard para visualiza√ß√£o
   - API REST para execu√ß√£o remota
   - Gr√°ficos de converg√™ncia em tempo real

## üìö Recursos Adicionais

### Comandos de Refer√™ncia R√°pida

```bash
# EXECU√á√ÉO LOCAL
make test                    # Teste b√°sico
make test-scaling           # An√°lise de performance
mpirun -np 8 ./build/monte_carlo_pi 10000000

# EXECU√á√ÉO DOCKER  
make docker-deploy          # Setup completo
make docker-run             # Executar distribu√≠do
make docker-menu            # Interface interativa
./scripts/run_docker.sh run 16 100000000

# UTILIT√ÅRIOS
make clean                  # Limpar arquivos
make check-mpi             # Verificar MPI
make mpi-info              # Info do sistema
make help                  # Ajuda completa
```

### Par√¢metros Recomendados

| Cen√°rio | Processos | Pontos | Tempo Aprox |
|---------|-----------|--------|-------------|
| Teste R√°pido | 4 | 100K | < 1s |
| Desenvolvimento | 8 | 1M | ~1s |
| Benchmark | 16 | 10M | ~10s |
| Produ√ß√£o | 32 | 100M+ | ~60s+ |

---

## üéØ Conclus√£o

Este projeto demonstra conceitos fundamentais de:
- **Programa√ß√£o Paralela** com MPI
- **M√©todos Monte Carlo** para computa√ß√£o cient√≠fica  
- **Containeriza√ß√£o** com Docker
- **Otimiza√ß√£o de Performance** em sistemas distribu√≠dos

**Ideal para:** Estudantes de computa√ß√£o paralela, desenvolvedores interessados em MPI, e profissionais que trabalham com simula√ß√µes num√©ricas.

**Tecnologias:** C, MPI, Docker, Alpine Linux, Shell Script, Makefile 