# Projeto MPI: CÃ¡lculo de Pi usando Monte Carlo

## DescriÃ§Ã£o do Problema

Este projeto implementa o cÃ¡lculo do valor de Pi (Ï€) utilizando o mÃ©todo Monte Carlo com programaÃ§Ã£o paralela atravÃ©s do MPI (Message Passing Interface). O mÃ©todo Monte Carlo Ã© uma tÃ©cnica estatÃ­stica que usa nÃºmeros aleatÃ³rios para resolver problemas matemÃ¡ticos, sendo ideal para demonstrar conceitos de computaÃ§Ã£o paralela.

### MÃ©todo Monte Carlo para CÃ¡lculo de Pi

O mÃ©todo baseia-se na relaÃ§Ã£o geomÃ©trica entre um cÃ­rculo inscrito em um quadrado:

1. **PrincÃ­pio**: Consideramos um cÃ­rculo de raio 1 inscrito em um quadrado de lado 2
2. **Ãrea do cÃ­rculo**: Ï€ Ã— rÂ² = Ï€ Ã— 1Â² = Ï€
3. **Ãrea do quadrado**: (2r)Â² = 4
4. **RazÃ£o**: Ï€/4 = (Ã¡rea do cÃ­rculo)/(Ã¡rea do quadrado)

**Algoritmo**:
- Gerar pontos aleatÃ³rios (x, y) no intervalo [-1, 1]
- Verificar se cada ponto estÃ¡ dentro do cÃ­rculo: xÂ² + yÂ² â‰¤ 1
- Calcular a razÃ£o: pontos_dentro_cÃ­rculo / total_pontos
- Estimar Ï€: 4 Ã— (pontos_dentro_cÃ­rculo / total_pontos)

## MPI: DistribuÃ­do vs Threads

### ğŸŒ **MPI Ã© para ComputaÃ§Ã£o DistribuÃ­da**
- **MPI usa PROCESSOS**, nÃ£o threads
- **Funciona em mÃºltiplas mÃ¡quinas** conectadas por rede
- **MemÃ³ria distribuÃ­da**: cada processo tem sua prÃ³pria memÃ³ria
- **ComunicaÃ§Ã£o via mensagens**: processos se comunicam enviando dados
- **EscalÃ¡vel para clusters e supercomputadores**

### ğŸ”§ **Verificar Slots DisponÃ­veis**

```bash
# Ver mapeamento de processos
mpirun --display-map --np 8 hostname

# Ver alocaÃ§Ã£o de recursos  
mpirun --display-allocation --np 4 hostname

# NÃºmero de cores do sistema
sysctl -n hw.ncpu  # macOS
nproc              # Linux
```

### ğŸŒ **ExecuÃ§Ã£o DistribuÃ­da**

#### Arquivo de Hosts (hostfile)
```bash
# Criar hostfile para mÃºltiplas mÃ¡quinas
echo "server1 slots=8" > hostfile
echo "server2 slots=4" >> hostfile
echo "192.168.1.10 slots=16" >> hostfile

# Executar distribuÃ­do
mpirun -np 28 --hostfile hostfile ./monte_carlo_pi 10000000
```

#### ConfiguraÃ§Ã£o SSH
```bash
# Configurar acesso sem senha
ssh-keygen -t rsa
ssh-copy-id user@server1
ssh-copy-id user@server2
```

## Plano da SoluÃ§Ã£o com MPI

### Arquitectura Paralela

**EstratÃ©gia de ParalelizaÃ§Ã£o**:
- **Processo Master (rank 0)**: Coordena a execuÃ§Ã£o, coleta resultados e calcula Pi final
- **Processos Worker (rank > 0)**: Executam simulaÃ§Ãµes Monte Carlo independentes
- **DistribuiÃ§Ã£o do trabalho**: Cada processo gera N/P pontos (onde N = total de pontos, P = nÃºmero de processos)

### Funcionalidades MPI Utilizadas

#### 1. **InicializaÃ§Ã£o e FinalizaÃ§Ã£o**
```c
MPI_Init(&argc, &argv)     // Inicializar ambiente MPI
MPI_Finalize()             // Finalizar ambiente MPI
```

#### 2. **IdentificaÃ§Ã£o de Processos**
```c
MPI_Comm_rank(MPI_COMM_WORLD, &rank)  // Identifica o rank do processo
MPI_Comm_size(MPI_COMM_WORLD, &size)  // ObtÃ©m nÃºmero total de processos
```

#### 3. **ComunicaÃ§Ã£o Point-to-Point**
```c
MPI_Send()  // Processos worker enviam resultados para master
MPI_Recv()  // Processo master recebe resultados dos workers
```

#### 4. **OperaÃ§Ãµes Coletivas**
```c
MPI_Reduce()     // ReduÃ§Ã£o para somar todos os pontos dentro do cÃ­rculo
MPI_Bcast()      // Broadcast do nÃºmero de pontos por processo (opcional)
```

#### 5. **MediÃ§Ã£o de Tempo**
```c
MPI_Wtime()  // Medir tempo de execuÃ§Ã£o para anÃ¡lise de performance
```

### Estrutura do Programa

#### Arquivos do Projeto

```
/
â”œâ”€â”€ README.md              # Este arquivo
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ monte_carlo_pi.c   # Programa principal MPI
â”‚   â””â”€â”€ utils.h            # FunÃ§Ãµes utilitÃ¡rias
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ compile.sh         # Script de compilaÃ§Ã£o
â”‚   â””â”€â”€ run.sh            # Script de execuÃ§Ã£o
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ hostfile           # Exemplo de configuraÃ§Ã£o distribuÃ­da
â”‚   â””â”€â”€ hostfile_simple    # ConfiguraÃ§Ã£o local simples
â”œâ”€â”€ results/
â”‚   â””â”€â”€ performance.txt    # Resultados de performance
â””â”€â”€ docs/
    â”œâ”€â”€ mpi_analysis.md    # AnÃ¡lise das funcionalidades MPI
    â””â”€â”€ mpi_distributed.md # Guia de execuÃ§Ã£o distribuÃ­da
```

### Algoritmo Detalhado

#### Processo Master (rank 0):
1. Inicializar MPI
2. Determinar nÃºmero de pontos por processo
3. Executar simulaÃ§Ã£o Monte Carlo local
4. Receber resultados dos processos worker
5. Calcular Pi final e tempo de execuÃ§Ã£o
6. Exibir resultados e estatÃ­sticas

#### Processos Worker (rank > 0):
1. Inicializar MPI
2. Receber nÃºmero de pontos para processar
3. Executar simulaÃ§Ã£o Monte Carlo local
4. Enviar resultado para processo master

### AnÃ¡lise de Performance

**MÃ©tricas a serem medidas**:
- Tempo de execuÃ§Ã£o sequencial vs paralelo
- Speedup: T_sequencial / T_paralelo
- EficiÃªncia: Speedup / nÃºmero_de_processos
- Escalabilidade com diferentes nÃºmeros de processos

**Testes Planejados**:
- ExecuÃ§Ã£o com 1, 2, 4, 8 processos
- Diferentes nÃºmeros de pontos (10â¶, 10â·, 10â¸)
- AnÃ¡lise da precisÃ£o vs. performance

### Funcionalidades MPI em Detalhes

#### 1. **MPI_Init e MPI_Finalize**
- **PropÃ³sito**: InicializaÃ§Ã£o e finalizaÃ§Ã£o do ambiente MPI
- **Uso**: ObrigatÃ³rio no inÃ­cio e fim de qualquer programa MPI

#### 2. **MPI_Comm_rank e MPI_Comm_size**
- **PropÃ³sito**: IdentificaÃ§Ã£o de processos e conhecimento do ambiente
- **Uso**: Determinar papel de cada processo (master/worker)

#### 3. **MPI_Send e MPI_Recv**
- **PropÃ³sito**: ComunicaÃ§Ã£o ponto-a-ponto bloqueante
- **Uso**: Workers enviam contagem de pontos para master

#### 4. **MPI_Reduce**
- **PropÃ³sito**: OperaÃ§Ã£o coletiva de reduÃ§Ã£o (soma)
- **Uso**: Somar todos os pontos dentro do cÃ­rculo de todos os processos
- **Vantagem**: Mais eficiente que mÃºltiplos Send/Recv

#### 5. **MPI_Wtime**
- **PropÃ³sito**: MediÃ§Ã£o precisa de tempo
- **Uso**: AnÃ¡lise de performance e benchmarking

### CompilaÃ§Ã£o e ExecuÃ§Ã£o

```bash
# CompilaÃ§Ã£o
mpicc -o monte_carlo_pi src/monte_carlo_pi.c -lm

# ExecuÃ§Ã£o local
mpirun -np 4 ./monte_carlo_pi 1000000

# ExecuÃ§Ã£o distribuÃ­da
mpirun -np 16 --hostfile hostfile ./monte_carlo_pi 10000000

# ExecuÃ§Ã£o com diferentes configuraÃ§Ãµes
mpirun -np 2 ./monte_carlo_pi 10000000
mpirun -np 8 ./monte_carlo_pi 100000000
```

### Usando o Makefile

```bash
# Compilar
make compile

# Testes rÃ¡pidos
make test              # Teste bÃ¡sico
make test-scaling      # Teste de escalabilidade  
make test-precision    # Teste de precisÃ£o

# Verificar sistema
make check-mpi         # Verificar se MPI estÃ¡ instalado
make mpi-info          # InformaÃ§Ãµes do sistema MPI

# Limpeza
make clean            # Remover arquivos gerados
```

### Resultados Esperados

**SaÃ­da do Programa**:
```
Calculando Pi usando Monte Carlo com MPI
NÃºmero de processos: 4
Pontos por processo: 250000
Total de pontos: 1000000

Processo 0: 196350 pontos dentro do cÃ­rculo
Processo 1: 196428 pontos dentro do cÃ­rculo  
Processo 2: 196502 pontos dentro do cÃ­rculo
Processo 3: 196381 pontos dentro do cÃ­rculo

Total de pontos dentro do cÃ­rculo: 785661
Pi estimado: 3.142644
Pi real: 3.141593
Erro: 0.033%
Tempo de execuÃ§Ã£o: 0.025 segundos
Speedup: 3.2x
EficiÃªncia: 80%
```

### Vantagens da SoluÃ§Ã£o MPI

1. **Escalabilidade**: Pode ser executado em mÃºltiplas mÃ¡quinas
2. **Flexibilidade**: Funciona em clusters e supercomputadores
3. **Portabilidade**: PadrÃ£o MPI Ã© amplamente suportado
4. **Performance**: DistribuiÃ§Ã£o eficiente do trabalho computacional

### Conceitos de MPI Demonstrados

- **SPMD** (Single Program, Multiple Data): Mesmo programa, dados diferentes
- **DecomposiÃ§Ã£o de domÃ­nio**: DivisÃ£o do problema em partes independentes
- **ComunicaÃ§Ã£o coletiva**: Uso eficiente de operaÃ§Ãµes MPI
- **SincronizaÃ§Ã£o**: CoordenaÃ§Ã£o entre processos
- **Load balancing**: DistribuiÃ§Ã£o equilibrada do trabalho

### ExtensÃµes Futuras

1. **MPI_Scatter/MPI_Gather**: Para distribuiÃ§Ã£o mais sofisticada
2. **MPI_Isend/MPI_Irecv**: ComunicaÃ§Ã£o nÃ£o-bloqueante
3. **MÃºltiplos communicators**: Para hierarquias de processos
4. **MPI-IO**: Para escrita paralela de resultados

Este projeto fornece uma base sÃ³lida para compreender os conceitos fundamentais do MPI e sua aplicaÃ§Ã£o em problemas de computaÃ§Ã£o cientÃ­fica.

# Utilizando Docker Swarm com MPI DistribuÃ­do

Este projeto demonstra como executar um programa paralelo com MPI (Message Passing Interface) utilizando mÃºltiplos containers Docker que atuam como nÃ³s de um cluster distribuÃ­do.

O cluster utiliza Docker Swarm para orquestraÃ§Ã£o e pode ser facilmente escalado conforme a necessidade. Cada nÃ³ executa em um container separado com todas as dependÃªncias MPI configuradas.

## Arquitetura do Cluster

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Container 1   â”‚    â”‚   Container 2   â”‚    â”‚   Container N   â”‚
â”‚   (mpi-node-1)  â”‚â—„â”€â”€â–ºâ”‚   (mpi-node-2)  â”‚â—„â”€â”€â–ºâ”‚   (mpi-node-N)  â”‚
â”‚   Ubuntu + MPI  â”‚    â”‚   Ubuntu + MPI  â”‚    â”‚   Ubuntu + MPI  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–²                       â–²                       â–²
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          Docker Swarm
                        Rede Overlay (mpi-net)
```

## ConfiguraÃ§Ã£o RÃ¡pida

### 1. Verificar dependÃªncias
```bash
make docker-check      # Verificar se Docker estÃ¡ instalado e funcionando
```

### 2. Construir e fazer deploy do cluster
```bash
make swarm-deploy      # ConstrÃ³i imagem otimizada e faz deploy no Swarm
```

### 3. Verificar status do cluster
```bash
make swarm-status      # Ver status dos containers
```

### 4. Executar programa distribuÃ­do
```bash
make swarm-run         # Executar Monte Carlo Pi no cluster
```

## Imagem Docker Otimizada

O projeto utiliza uma **imagem Alpine Linux multi-stage** altamente otimizada:

### ğŸš€ **Vantagens da OtimizaÃ§Ã£o**

| Aspecto | Ubuntu Original | **Alpine Otimizada** | **Melhoria** |
|---------|-----------------|---------------------|--------------|
| **Tamanho** | ~180MB | **~60MB** | **ğŸ”¥ 67% menor** |
| **Build Time** | Lento | **Muito RÃ¡pido** | **âš¡ 3x mais rÃ¡pido** |
| **SeguranÃ§a** | MÃ©dia | **Alta** | **ğŸ›¡ï¸ Menor superfÃ­cie de ataque** |
| **Recursos** | Altos | **MÃ­nimos** | **ğŸ’¾ Menos CPU/RAM** |

### ğŸ—ï¸ **Tecnologia Multi-stage**

A imagem utiliza build em **2 estÃ¡gios**:

```dockerfile
# EstÃ¡gio 1: CompilaÃ§Ã£o (descartado)
FROM alpine:3.19 AS builder
# Instala ferramentas de build (gcc, g++, make)
# Compila o programa MPI

# EstÃ¡gio 2: Runtime (imagem final)  
FROM alpine:3.19
# Instala apenas runtime MPI + SSH
# Copia apenas o binÃ¡rio compilado
```

**Resultado**: Imagem final contÃ©m apenas o necessÃ¡rio para executar, sem ferramentas de compilaÃ§Ã£o.

## Comandos Detalhados

### ConstruÃ§Ã£o e Deploy
```bash
make docker-build      # Construir imagem otimizada
make swarm-init        # Inicializar Docker Swarm
make swarm-deploy      # Deploy completo (build + init + deploy)
```

### OperaÃ§Ã£o do Cluster
```bash
make swarm-status      # Status dos serviÃ§os e containers
make swarm-test        # Testar conectividade entre nÃ³s
make swarm-run         # Executar programa MPI distribuÃ­do
make swarm-scale       # Escalar nÃºmero de nÃ³s interativamente
```

### Limpeza
```bash
make swarm-cleanup     # Remover stack do Swarm
make clean            # Limpar arquivos locais
```

## ConfiguraÃ§Ã£o Manual AvanÃ§ada

### 1. Personalizar nÃºmero de rÃ©plicas

Edite o arquivo `docker/docker-compose.yml`:
```yaml
services:
  mpi-node:
    # ... existing code ...
    deploy:
      replicas: 8  # Altere para o nÃºmero desejado de nÃ³s
```

### 2. Executar comandos personalizados no cluster

Acesse um container:
```bash
# Listar containers
docker ps | grep mpi_stack

# Acessar container especÃ­fico
docker exec -u mpiuser -it <container_name> bash

# Executar MPI personalizado
mpirun -np 16 --host mpi-node-1,mpi-node-2,... ./monte_carlo_pi 1000000000
```

### 3. Monitoramento do cluster

```bash
# Ver logs dos serviÃ§os
docker service logs mpi_stack_mpi-node

# Monitorar recursos
docker stats

# Inspecionar rede
docker network ls
docker network inspect mpi_stack_mpi-net
```

## Vantagens do Docker Swarm

1. **Facilidade de uso**: Deploy com um comando
2. **Escalabilidade**: Adicione/remova nÃ³s facilmente
3. **Isolamento**: Cada processo MPI roda em container isolado
4. **Portabilidade**: Funciona em qualquer ambiente com Docker
5. **Reprodutibilidade**: Ambiente idÃªntico em qualquer mÃ¡quina
6. **OrquestraÃ§Ã£o nativa**: Gerenciamento automÃ¡tico de containers

## SoluÃ§Ã£o de Problemas

### Problema: Docker Swarm nÃ£o inicializado
```bash
docker swarm init
```

### Problema: Containers nÃ£o se comunicam
```bash
# Verificar rede overlay
docker network ls | grep overlay

# Testar conectividade
make swarm-test
```

### Problema: Imagem nÃ£o encontrada
```bash
# Reconstruir imagem
make docker-build
```

### Problema: Stack nÃ£o remove
```bash
# ForÃ§ar remoÃ§Ã£o
docker stack rm mpi_stack
docker system prune -f
```

## ComparaÃ§Ã£o: Local vs Docker Swarm

| Aspecto | ExecuÃ§Ã£o Local | Docker Swarm |
|---------|----------------|--------------|
| Setup | Instalar MPI localmente | Docker + 1 comando |
| Escalabilidade | Limitada aos cores locais | Ilimitada (mÃºltiplas mÃ¡quinas) |
| Isolamento | Processos compartilham OS | Containers isolados |
| Portabilidade | Dependente do SO | Funciona em qualquer Docker |
| Overhead | MÃ­nimo | Pequeno (containers) |
| Gerenciamento | Manual | AutomÃ¡tico (Swarm) |

## Conceitos de MPI Demonstrados

- **SPMD** (Single Program, Multiple Data): Mesmo programa, dados diferentes
- **DecomposiÃ§Ã£o de domÃ­nio**: DivisÃ£o do problema em partes independentes
- **ComunicaÃ§Ã£o coletiva**: Uso eficiente de operaÃ§Ãµes MPI
- **SincronizaÃ§Ã£o**: CoordenaÃ§Ã£o entre processos
- **Load balancing**: DistribuiÃ§Ã£o equilibrada do trabalho
- **Cluster computing**: ExecuÃ§Ã£o em mÃºltiplos nÃ³s fÃ­sicos/virtuais

Este projeto fornece uma base sÃ³lida para compreender tanto os conceitos fundamentais do MPI quanto sua aplicaÃ§Ã£o em ambientes containerizados e distribuÃ­dos. 