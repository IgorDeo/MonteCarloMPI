# Projeto MPI: CÃ¡lculo de Pi usando Monte Carlo

## DescriÃ§Ã£o do Problema

Este projeto implementa o cÃ¡lculo do valor de Pi (Ï€) utilizando o mÃ©todo Monte Carlo com programaÃ§Ã£o paralela atravÃ©s do MPI (Message Passing Interface). O mÃ©todo Monte Carlo Ã© uma tÃ©cnica estatÃ­stica que usa nÃºmeros aleatÃ³rios para resolver problemas matemÃ¡ticos, sendo ideal para demonstrar conceitos de computaÃ§Ã£o paralela.

### MÃ©todo Monte Carlo para CÃ¡lculo de Pi

O mÃ©todo baseia-se na relaÃ§Ã£o geomÃ©trica entre um cÃ­rculo inscrito em um quadrado:

1. **PrincÃ­pio**: Consideramos um cÃ­rculo de raio 1 inscrito em um quadrado de lado 2
2. **Ãrea do cÃ­rculo**: Ï€ Ã— rÂ² = Ï€ Ã— 1Â² = Ï€
3. **Ãrea do quadrado**: (2r)Â² = 4
4. **RazÃ£o**: Ï€/4 = (Ã¡rea do cÃ­rculo)/(Ã¡rea do quadrado)

**Algoritmo**:
- Gerar pontos aleatÃ³rios (x, y) no intervalo [-1, 1]
- Verificar se cada ponto estÃ¡ dentro do cÃ­rculo: xÂ² + yÂ² â‰¤ 1
- Calcular a razÃ£o: pontos_dentro_cÃ­rculo / total_pontos
- Estimar Ï€: 4 Ã— (pontos_dentro_cÃ­rculo / total_pontos)

## MPI: DistribuÃ­do vs Threads

### ğŸŒ **MPI Ã© para ComputaÃ§Ã£o DistribuÃ­da**
- **MPI usa PROCESSOS**, nÃ£o threads
- **Funciona em mÃºltiplas mÃ¡quinas** conectadas por rede
- **MemÃ³ria distribuÃ­da**: cada processo tem sua prÃ³pria memÃ³ria
- **ComunicaÃ§Ã£o via mensagens**: processos se comunicam enviando dados
- **EscalÃ¡vel para clusters e supercomputadores**

### ğŸ”§ **Verificar Slots DisponÃ­veis**

```bash
# Ver mapeamento de processos
mpirun --display-map --np 8 hostname

# Ver alocaÃ§Ã£o de recursos  
mpirun --display-allocation --np 4 hostname

# NÃºmero de cores do sistema
sysctl -n hw.ncpu  # macOS
nproc              # Linux
```

### ğŸŒ **ExecuÃ§Ã£o DistribuÃ­da**

#### Arquivo de Hosts (hostfile)
```bash
# Criar hostfile para mÃºltiplas mÃ¡quinas
echo "server1 slots=8" > hostfile
echo "server2 slots=4" >> hostfile
echo "192.168.1.10 slots=16" >> hostfile

# Executar distribuÃ­do
mpirun -np 28 --hostfile hostfile ./monte_carlo_pi 10000000
```

#### ConfiguraÃ§Ã£o SSH
```bash
# Configurar acesso sem senha
ssh-keygen -t rsa
ssh-copy-id user@server1
ssh-copy-id user@server2
```

## Plano da SoluÃ§Ã£o com MPI

### Arquitectura Paralela

**EstratÃ©gia de ParalelizaÃ§Ã£o**:
- **Processo Master (rank 0)**: Coordena a execuÃ§Ã£o, coleta resultados e calcula Pi final
- **Processos Worker (rank > 0)**: Executam simulaÃ§Ãµes Monte Carlo independentes
- **DistribuiÃ§Ã£o do trabalho**: Cada processo gera N/P pontos (onde N = total de pontos, P = nÃºmero de processos)

### Funcionalidades MPI Utilizadas

#### 1. **InicializaÃ§Ã£o e FinalizaÃ§Ã£o**
```c
MPI_Init(&argc, &argv)     // Inicializar ambiente MPI
MPI_Finalize()             // Finalizar ambiente MPI
```

#### 2. **IdentificaÃ§Ã£o de Processos**
```c
MPI_Comm_rank(MPI_COMM_WORLD, &rank)  // Identifica o rank do processo
MPI_Comm_size(MPI_COMM_WORLD, &size)  // ObtÃ©m nÃºmero total de processos
```

#### 3. **ComunicaÃ§Ã£o Point-to-Point**
```c
MPI_Send()  // Processos worker enviam resultados para master
MPI_Recv()  // Processo master recebe resultados dos workers
```

#### 4. **OperaÃ§Ãµes Coletivas**
```c
MPI_Reduce()     // ReduÃ§Ã£o para somar todos os pontos dentro do cÃ­rculo
MPI_Bcast()      // Broadcast do nÃºmero de pontos por processo (opcional)
```

#### 5. **MediÃ§Ã£o de Tempo**
```c
MPI_Wtime()  // Medir tempo de execuÃ§Ã£o para anÃ¡lise de performance
```

### Estrutura do Programa

#### Arquivos do Projeto

```
/
â”œâ”€â”€ README.md              # Este arquivo
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ monte_carlo_pi.c   # Programa principal MPI
â”‚   â””â”€â”€ utils.h            # FunÃ§Ãµes utilitÃ¡rias
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ compile.sh         # Script de compilaÃ§Ã£o
â”‚   â””â”€â”€ run.sh            # Script de execuÃ§Ã£o
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ hostfile           # Exemplo de configuraÃ§Ã£o distribuÃ­da
â”‚   â””â”€â”€ hostfile_simple    # ConfiguraÃ§Ã£o local simples
â”œâ”€â”€ results/
â”‚   â””â”€â”€ performance.txt    # Resultados de performance
â””â”€â”€ docs/
    â”œâ”€â”€ mpi_analysis.md    # AnÃ¡lise das funcionalidades MPI
    â””â”€â”€ mpi_distributed.md # Guia de execuÃ§Ã£o distribuÃ­da
```

### Algoritmo Detalhado

#### Processo Master (rank 0):
1. Inicializar MPI
2. Determinar nÃºmero de pontos por processo
3. Executar simulaÃ§Ã£o Monte Carlo local
4. Receber resultados dos processos worker
5. Calcular Pi final e tempo de execuÃ§Ã£o
6. Exibir resultados e estatÃ­sticas

#### Processos Worker (rank > 0):
1. Inicializar MPI
2. Receber nÃºmero de pontos para processar
3. Executar simulaÃ§Ã£o Monte Carlo local
4. Enviar resultado para processo master

### AnÃ¡lise de Performance

**MÃ©tricas a serem medidas**:
- Tempo de execuÃ§Ã£o sequencial vs paralelo
- Speedup: T_sequencial / T_paralelo
- EficiÃªncia: Speedup / nÃºmero_de_processos
- Escalabilidade com diferentes nÃºmeros de processos

**Testes Planejados**:
- ExecuÃ§Ã£o com 1, 2, 4, 8 processos
- Diferentes nÃºmeros de pontos (10â¶, 10â·, 10â¸)
- AnÃ¡lise da precisÃ£o vs. performance

### Funcionalidades MPI em Detalhes

#### 1. **MPI_Init e MPI_Finalize**
- **PropÃ³sito**: InicializaÃ§Ã£o e finalizaÃ§Ã£o do ambiente MPI
- **Uso**: ObrigatÃ³rio no inÃ­cio e fim de qualquer programa MPI

#### 2. **MPI_Comm_rank e MPI_Comm_size**
- **PropÃ³sito**: IdentificaÃ§Ã£o de processos e conhecimento do ambiente
- **Uso**: Determinar papel de cada processo (master/worker)

#### 3. **MPI_Send e MPI_Recv**
- **PropÃ³sito**: ComunicaÃ§Ã£o ponto-a-ponto bloqueante
- **Uso**: Workers enviam contagem de pontos para master

#### 4. **MPI_Reduce**
- **PropÃ³sito**: OperaÃ§Ã£o coletiva de reduÃ§Ã£o (soma)
- **Uso**: Somar todos os pontos dentro do cÃ­rculo de todos os processos
- **Vantagem**: Mais eficiente que mÃºltiplos Send/Recv

#### 5. **MPI_Wtime**
- **PropÃ³sito**: MediÃ§Ã£o precisa de tempo
- **Uso**: AnÃ¡lise de performance e benchmarking

### CompilaÃ§Ã£o e ExecuÃ§Ã£o

```bash
# CompilaÃ§Ã£o
mpicc -o monte_carlo_pi src/monte_carlo_pi.c -lm

# ExecuÃ§Ã£o local
mpirun -np 4 ./monte_carlo_pi 1000000

# ExecuÃ§Ã£o distribuÃ­da
mpirun -np 16 --hostfile hostfile ./monte_carlo_pi 10000000

# ExecuÃ§Ã£o com diferentes configuraÃ§Ãµes
mpirun -np 2 ./monte_carlo_pi 10000000
mpirun -np 8 ./monte_carlo_pi 100000000
```

### Usando o Makefile

```bash
# Compilar
make compile

# Testes rÃ¡pidos
make test              # Teste bÃ¡sico
make test-scaling      # Teste de escalabilidade  
make test-precision    # Teste de precisÃ£o

# Verificar sistema
make check-mpi         # Verificar se MPI estÃ¡ instalado
make mpi-info          # InformaÃ§Ãµes do sistema MPI

# Limpeza
make clean            # Remover arquivos gerados
```

### Resultados Esperados

**SaÃ­da do Programa**:
```
Calculando Pi usando Monte Carlo com MPI
NÃºmero de processos: 4
Pontos por processo: 250000
Total de pontos: 1000000

Processo 0: 196350 pontos dentro do cÃ­rculo
Processo 1: 196428 pontos dentro do cÃ­rculo  
Processo 2: 196502 pontos dentro do cÃ­rculo
Processo 3: 196381 pontos dentro do cÃ­rculo

Total de pontos dentro do cÃ­rculo: 785661
Pi estimado: 3.142644
Pi real: 3.141593
Erro: 0.033%
Tempo de execuÃ§Ã£o: 0.025 segundos
Speedup: 3.2x
EficiÃªncia: 80%
```

### Vantagens da SoluÃ§Ã£o MPI

1. **Escalabilidade**: Pode ser executado em mÃºltiplas mÃ¡quinas
2. **Flexibilidade**: Funciona em clusters e supercomputadores
3. **Portabilidade**: PadrÃ£o MPI Ã© amplamente suportado
4. **Performance**: DistribuiÃ§Ã£o eficiente do trabalho computacional

### Conceitos de MPI Demonstrados

- **SPMD** (Single Program, Multiple Data): Mesmo programa, dados diferentes
- **DecomposiÃ§Ã£o de domÃ­nio**: DivisÃ£o do problema em partes independentes
- **ComunicaÃ§Ã£o coletiva**: Uso eficiente de operaÃ§Ãµes MPI
- **SincronizaÃ§Ã£o**: CoordenaÃ§Ã£o entre processos
- **Load balancing**: DistribuiÃ§Ã£o equilibrada do trabalho

### ExtensÃµes Futuras

1. **MPI_Scatter/MPI_Gather**: Para distribuiÃ§Ã£o mais sofisticada
2. **MPI_Isend/MPI_Irecv**: ComunicaÃ§Ã£o nÃ£o-bloqueante
3. **MÃºltiplos communicators**: Para hierarquias de processos
4. **MPI-IO**: Para escrita paralela de resultados

Este projeto fornece uma base sÃ³lida para compreender os conceitos fundamentais do MPI e sua aplicaÃ§Ã£o em problemas de computaÃ§Ã£o cientÃ­fica.

# Utilizando Docker com MPI DistribuÃ­do
Este projeto demonstra como executar um programa paralelo com MPI (Message Passing Interface) utilizando quatro containers Docker que atuam como nÃ³s de um cluster.

Os containers sÃ£o definidos no docker-compose.yml e compartilham uma mesma rede Docker personalizada (mpi-net), permitindo que se comuniquem diretamente pelos nomes de host (mpi-node1, mpi-node2, etc.). Cada container Ã© configurado com:

- OpenSSH Server para permitir acesso remoto via SSH.

- MPICH (implementaÃ§Ã£o do MPI).

- Um usuÃ¡rio comum (mpiuser) com autenticaÃ§Ã£o via chave SSH para permitir comunicaÃ§Ã£o sem senha entre os nÃ³s.

## 1. Build da imagem
Para construir a imagem base utilizada pelos nÃ³s, rode o comando `docker-compose build`. Isso criarÃ¡ os containers com todas as dependÃªncias do MPI e SSH.

## 2 Mudar para o usuÃ¡rio mpiuser

Para facilitar os testes de conectividade SSH entre os containers, utilize o usuÃ¡rio mpiuser, que jÃ¡ estÃ¡ configurado com as chaves pÃºblicas e senha padrÃ£o mpiuser.

Acesse o terminal do container mpi-node1 e mude para o usuÃ¡rio:

```bash
docker exec -it mpi-node1 bash
su - mpiuser
```

Com isso, todos os testes seguintes (como ping, ssh e mpirun) devem ser executados a partir desse usuÃ¡rio.

## 3. Testar conectividade e SSH de mpi-node1 para os outros nÃ³s

### 3.1 Verificar conectividade via ping

```bash
for host in mpi-node1 mpi-node2 mpi-node3 mpi-node4; do
  echo "Pingando $host..."
  ping -c 1 -W 1 $host && echo "Ping OK" || echo "Ping falhou"
done
```
Isso confirma que todos os nÃ³s estÃ£o acessÃ­veis pela rede Docker.

### 3.2 Testar conexÃ£o SSH
O comando abaixo tenta conectar via SSH a partir de mpi-node1 e evita o prompt interativo da primeira conexÃ£o:

```bash
for host in mpi-node1 mpi-node2 mpi-node3 mpi-node4; do
  echo "Testando SSH em $host..."
  ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 mpiuser@$host "echo 'ConexÃ£o bem sucedida a $host'" || echo "Falha SSH em $host"
done
```

Com isso os hosts tambÃ©m sÃ£o adicionados ao `known_hosts`, permitindo executar o MPI distribuÃ­do.

## 4. Executar o MPI distribuÃ­do
Uma vez que os testes de rede e SSH estejam funcionando:

```bash
mpirun -np 4 --host mpi-node1,mpi-node2,mpi-node3,mpi-node4 /home/mpiuser/monte_carlo_pi 1000000000
```
Esse comando executa o programa monte_carlo_pi de forma distribuÃ­da nos 4 nÃ³s.

# ğŸŒ¥ï¸ **Cluster HÃ­brido AWS (AvanÃ§ado)**

Pode executar seu programa distribuÃ­do entre **sua mÃ¡quina local e 1 instÃ¢ncia EC2** na nuvem!

### ConfiguraÃ§Ã£o RÃ¡pida
```bash
# 1. Instalar AWS CLI e configurar
pip install awscli
aws configure

# 2. Configurar cluster automaticamente
make aws-setup

# 3. Executar distribuÃ­do (10 processos: 8 local + 2 EC2)
make aws-run
```

### Arquitetura do Cluster
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Sua MÃ¡quina   â”‚    â”‚     EC2-1       â”‚
â”‚   (macOS)       â”‚    â”‚   (Ubuntu)      â”‚
â”‚   8 slots       â”‚â—„â”€â”€â–ºâ”‚   2 slots       â”‚
â”‚                 â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Custo Estimado
- **1 instÃ¢ncia t3.medium**: ~$0.04/hora
- **Teste de 1 dia**: ~$1.00
- **Muito econÃ´mico** para demonstraÃ§Ã£o de MPI distribuÃ­do

### Comandos Ãšteis
```bash
make aws-check      # Verificar dependÃªncias
make aws-setup      # Configurar cluster completo
make aws-test       # Testar conectividade
make aws-run        # Executar programa distribuÃ­do
make aws-cleanup    # Terminar instÃ¢ncia EC2
```

> ğŸ“– **Manual completo**: `docs/aws_cluster_setup.md` 